{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b219ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   loan_amnt  int_rate  installment  annual_inc    dti  delinq_2yrs  pub_rec  \\\n",
      "0     2500.0     13.98        85.42     20004.0  19.86          0.0      0.0   \n",
      "1     5000.0     15.95       175.67     59000.0  19.57          0.0      0.0   \n",
      "2     7000.0      9.91       225.58     53796.0  10.80          3.0      0.0   \n",
      "3     2000.0      5.42        60.32     30000.0   3.60          0.0      0.0   \n",
      "4     3600.0     10.25       116.59    675048.0   1.55          0.0      0.0   \n",
      "\n",
      "   long_term  employment_verified  employment  housing_instability  \\\n",
      "0          0                    0           1                    0   \n",
      "1          0                    0           1                    0   \n",
      "2          0                    0           1                    0   \n",
      "3          0                    0           1                    0   \n",
      "4          0                    0           1                    0   \n",
      "\n",
      "   log_i2p_ratio  abandonment  \n",
      "0       0.207108            0  \n",
      "1       0.376878            1  \n",
      "2       0.143777            0  \n",
      "3       0.077729            0  \n",
      "4       0.155585            0  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 37562 entries, 0 to 37561\n",
      "Data columns (total 13 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   loan_amnt            37562 non-null  float64\n",
      " 1   int_rate             37562 non-null  float64\n",
      " 2   installment          37562 non-null  float64\n",
      " 3   annual_inc           37562 non-null  float64\n",
      " 4   dti                  37562 non-null  float64\n",
      " 5   delinq_2yrs          37562 non-null  float64\n",
      " 6   pub_rec              37562 non-null  float64\n",
      " 7   long_term            37562 non-null  int64  \n",
      " 8   employment_verified  37562 non-null  int64  \n",
      " 9   employment           37562 non-null  int64  \n",
      " 10  housing_instability  37562 non-null  int64  \n",
      " 11  log_i2p_ratio        37562 non-null  float64\n",
      " 12  abandonment          37562 non-null  int64  \n",
      "dtypes: float64(8), int64(5)\n",
      "memory usage: 3.7 MB\n",
      "None\n",
      "loan_amnt              0\n",
      "int_rate               0\n",
      "installment            0\n",
      "annual_inc             0\n",
      "dti                    0\n",
      "delinq_2yrs            0\n",
      "pub_rec                0\n",
      "long_term              0\n",
      "employment_verified    0\n",
      "employment             0\n",
      "housing_instability    0\n",
      "log_i2p_ratio          0\n",
      "abandonment            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = '../datasets/processed_loan_data.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(data.head())\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(data.info())\n",
    "\n",
    "# Check for missing values\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d363a2a6",
   "metadata": {},
   "source": [
    "Comencemos con un modelo de regresion logistica, vamos a comparar que pasa cuando utilizamos o no utilizamos `class_weight='balanced'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03258737",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[6305    8]\n",
      " [ 261  939]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98      6313\n",
      "           1       0.99      0.78      0.87      1200\n",
      "\n",
      "    accuracy                           0.96      7513\n",
      "   macro avg       0.98      0.89      0.93      7513\n",
      "weighted avg       0.97      0.96      0.96      7513\n",
      "\n",
      "\n",
      "ROC-AUC Score:\n",
      "0.9785536459158352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hseba\\anaconda3\\envs\\data_science\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = data.drop(columns=['abandonment'])\n",
    "y = data['abandonment']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the Logistic Regression model\n",
    "log_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = log_reg.predict(X_test)\n",
    "y_pred_proba = log_reg.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\nROC-AUC Score:\")\n",
    "print(roc_auc_score(y_test, y_pred_proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e2791c2",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[6285   28]\n",
      " [ 114 1086]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      6313\n",
      "           1       0.97      0.91      0.94      1200\n",
      "\n",
      "    accuracy                           0.98      7513\n",
      "   macro avg       0.98      0.95      0.96      7513\n",
      "weighted avg       0.98      0.98      0.98      7513\n",
      "\n",
      "\n",
      "ROC-AUC Score:\n",
      "0.9838101272506468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hseba\\anaconda3\\envs\\data_science\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = data.drop(columns=['abandonment'])\n",
    "y = data['abandonment']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the Logistic Regression model\n",
    "log_reg = LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced')\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = log_reg.predict(X_test)\n",
    "y_pred_proba = log_reg.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\nROC-AUC Score:\")\n",
    "print(roc_auc_score(y_test, y_pred_proba))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c2c203",
   "metadata": {},
   "source": [
    "Al decirle al modelo que nuestras clases no están balanceadas, podemos ver como mejoró de manera significativa el recall de los abandonos (0.91 vs. 0.78). Veamos si podemos encontrar un modelo que tenga un desempeño mejor. Utilicemos RandomForestClassifier y GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7a0cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Results:\n",
      "Confusion Matrix:\n",
      "[[6301   12]\n",
      " [  78 1122]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      6313\n",
      "           1       0.99      0.94      0.96      1200\n",
      "\n",
      "    accuracy                           0.99      7513\n",
      "   macro avg       0.99      0.97      0.98      7513\n",
      "weighted avg       0.99      0.99      0.99      7513\n",
      "\n",
      "\n",
      "ROC-AUC Score:\n",
      "0.9961695443265219\n",
      "\n",
      "Gradient Boosting Results:\n",
      "Confusion Matrix:\n",
      "[[6307    6]\n",
      " [ 103 1097]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      6313\n",
      "           1       0.99      0.91      0.95      1200\n",
      "\n",
      "    accuracy                           0.99      7513\n",
      "   macro avg       0.99      0.96      0.97      7513\n",
      "weighted avg       0.99      0.99      0.99      7513\n",
      "\n",
      "\n",
      "ROC-AUC Score:\n",
      "0.9941833650139923\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "# Random Forest Classifier\n",
    "rf_clf = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "rf_clf.fit(X_train, y_train)\n",
    "rf_pred = rf_clf.predict(X_test)\n",
    "rf_pred_proba = rf_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"Random Forest Results:\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, rf_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, rf_pred))\n",
    "print(\"\\nROC-AUC Score:\")\n",
    "print(roc_auc_score(y_test, rf_pred_proba))\n",
    "\n",
    "# Gradient Boosting Classifier\n",
    "gb_clf = GradientBoostingClassifier(random_state=42)\n",
    "gb_clf.fit(X_train, y_train)\n",
    "gb_pred = gb_clf.predict(X_test)\n",
    "gb_pred_proba = gb_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"\\nGradient Boosting Results:\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, gb_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, gb_pred))\n",
    "print(\"\\nROC-AUC Score:\")\n",
    "print(roc_auc_score(y_test, gb_pred_proba))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992835e3",
   "metadata": {},
   "source": [
    "Random Forest es mejor que Gradient Boosting, el F1-Score en la clase de interés es de 0.96. Ahora quiero realizar un gridsearch para ver si encuentro un grupo de hiperparámetros que puedan darme un modelo aún mejor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8055fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 216 candidates, totalling 648 fits\n",
      "Best Parameters: {'class_weight': 'balanced', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 300}\n",
      "Best ROC-AUC Score: 0.9946725188524402\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'class_weight': ['balanced', 'balanced_subsample']\n",
    "}\n",
    "\n",
    "# Comenzar el GridSearchCV para el Random Forest\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    scoring='roc_auc',\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Mostrar los mejores parámetros y el mejor score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best ROC-AUC Score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1af205",
   "metadata": {},
   "source": [
    "El mejor ROC-AUC score del GridSearch es inferior (ligeramente) al que encontramos al inicio. Entonces nos quedaremos con el modelo que ya establecimos antes. Por curiosidad, quisiera validar mi decisión de generar la columna log_i2p_ratio para capturar que tanto ha pagado un usuario de interés vs. que tanto a pagado del monto principal, entonces voy a revisar feature_importances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "489e1260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loan_amnt: 0.0248\n",
      "int_rate: 0.1224\n",
      "installment: 0.0338\n",
      "annual_inc: 0.0322\n",
      "dti: 0.0316\n",
      "delinq_2yrs: 0.0041\n",
      "pub_rec: 0.0026\n",
      "long_term: 0.0468\n",
      "employment_verified: 0.0030\n",
      "employment: 0.0000\n",
      "housing_instability: 0.0000\n",
      "log_i2p_ratio: 0.6986\n"
     ]
    }
   ],
   "source": [
    "# Print feature importances\n",
    "feature_importances = rf_clf.feature_importances_\n",
    "for feature, importance in zip(X.columns, feature_importances):\n",
    "    print(f\"{feature}: {importance:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
